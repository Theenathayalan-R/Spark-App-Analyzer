<!DOCTYPE html><html><head><meta charset="utf-8"><title>Spark Application Analysis Report</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js"></script><style>body { font-family: Inter, Arial, sans-serif; background: #f8f9fa; color: #222; margin: 0; padding: 0; }.navbar { background: #fff; padding: 1rem 2rem; box-shadow: 0 2px 4px rgba(0,0,0,0.07); position: sticky; top: 0; z-index: 100; }.nav-links { display: flex; gap: 2rem; margin-top: 1rem; }.nav-links a { color: #222; text-decoration: none; font-weight: 500; padding: 0.5rem 1rem; border-radius: 8px; transition: background 0.2s; }.nav-links a:hover { background: #e3eafc; }.container { max-width: 1200px; margin: 2rem auto; background: #fff; border-radius: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 2rem; }.summary-cards { display: flex; gap: 2rem; margin-bottom: 2rem; }.summary-card { flex: 1; background: #f5f6fa; border-radius: 8px; padding: 1.5rem; box-shadow: 0 1px 2px rgba(0,0,0,0.03); text-align: center; }.section { margin-bottom: 2.5rem; }.section h2 { margin-top: 0; }.bottleneck, .failed, .slow { background: #fff3cd; border-left: 4px solid #e67e22; margin: 1rem 0; padding: 1rem; border-radius: 6px; }.recommendation { background: #e3eafc; border-left: 4px solid #3498db; margin: 1rem 0; padding: 1rem; border-radius: 6px; }.pyspark-issue { background: #f0f8ff; border-left: 4px solid #4169e1; margin: 1rem 0; padding: 1.5rem; border-radius: 6px; }.pyspark-issue h4 { margin: 0 0 10px 0; color: #4169e1; }.severity-high { border-left-color: #ff4444; background: #ffe6e6; }.severity-medium { border-left-color: #ff8800; background: #fff2e6; }.severity-low { border-left-color: #44aa44; background: #e6ffe6; }.code-fix { background: #f8f8f8; border: 1px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px; font-family: "Fira Code", "Consolas", monospace; font-size: 13px; overflow-x: auto; }.job-table { width: 100%; border-collapse: collapse; margin-top: 1rem; }.job-table th, .job-table td { border: 1px solid #eee; padding: 0.5rem 0.75rem; text-align: left; }.job-table th { background: #f5f6fa; }.performance-issue { background: #fff9e6; border-left: 4px solid #ffa500; margin: 1rem 0; padding: 1rem; border-radius: 6px; }</style></head><body><div class="navbar"><h1><i class="fas fa-chart-line"></i> Spark Application Analysis</h1><div class="nav-links"><a href="#summary">Summary</a><a href="#pyspark-issues">PySpark Issues</a><a href="#performance-issues">Performance Issues</a><a href="#bottlenecks">Bottlenecks</a><a href="#failures">Failures</a><a href="#recommendations">Recommendations</a></div></div><div class="container"><section id="summary" class="section"><h2>Summary</h2><div class="summary-cards"><div class="summary-card"><h3>Total Jobs</h3><div>3</div></div><div class="summary-card"><h3>Avg Duration</h3><div>9.67 s</div></div><div class="summary-card"><h3>Success Rate</h3><div>100.0%</div></div><div class="summary-card"><h3>PySpark Issues</h3><div>3</div></div><div class="summary-card"><h3>Performance Issues</h3><div>3</div></div></div></section><section id="pyspark-issues" class="section"><h2><i class="fab fa-python"></i> PySpark Code Issues</h2><div class="pyspark-issue severity-medium"><h4>[MEDIUM] High Gc Overhead</h4><p><strong>Problem:</strong> Executor 1 has high GC overhead (13.3% of execution time)</p><p><strong>Impact:</strong> Medium - Reduces CPU overhead from garbage collection</p><p><strong>Fix:</strong> Optimize memory usage and GC settings to reduce garbage collection overhead.</p><div class="code-fix"><pre><code class="language-python"># Reduce GC overhead:
spark.conf.set("spark.executor.memory", "8g")

spark.conf.set("spark.memory.fraction", "0.8")
spark.conf.set("spark.memory.storageFraction", "0.3")

spark.conf.set("spark.executor.extraJavaOptions", "-XX:+UseG1GC -XX:MaxGCPauseMillis=200")

df.mapPartitions(lambda partition: process_partition(partition))  # Process in batches</code></pre></div></div><div class="pyspark-issue severity-medium"><h4>[MEDIUM] High Gc Overhead</h4><p><strong>Problem:</strong> Executor 2 has high GC overhead (16.0% of execution time)</p><p><strong>Impact:</strong> Medium - Reduces CPU overhead from garbage collection</p><p><strong>Fix:</strong> Optimize memory usage and GC settings to reduce garbage collection overhead.</p><div class="code-fix"><pre><code class="language-python"># Reduce GC overhead:
spark.conf.set("spark.executor.memory", "8g")

spark.conf.set("spark.memory.fraction", "0.8")
spark.conf.set("spark.memory.storageFraction", "0.3")

spark.conf.set("spark.executor.extraJavaOptions", "-XX:+UseG1GC -XX:MaxGCPauseMillis=200")

df.mapPartitions(lambda partition: process_partition(partition))  # Process in batches</code></pre></div></div><div class="pyspark-issue severity-medium"><h4>[MEDIUM] High Gc Overhead</h4><p><strong>Problem:</strong> Executor 3 has high GC overhead (13.3% of execution time)</p><p><strong>Impact:</strong> Medium - Reduces CPU overhead from garbage collection</p><p><strong>Fix:</strong> Optimize memory usage and GC settings to reduce garbage collection overhead.</p><div class="code-fix"><pre><code class="language-python"># Reduce GC overhead:
spark.conf.set("spark.executor.memory", "8g")

spark.conf.set("spark.memory.fraction", "0.8")
spark.conf.set("spark.memory.storageFraction", "0.3")

spark.conf.set("spark.executor.extraJavaOptions", "-XX:+UseG1GC -XX:MaxGCPauseMillis=200")

df.mapPartitions(lambda partition: process_partition(partition))  # Process in batches</code></pre></div></div></section><section id="performance-issues" class="section"><h2><i class="fas fa-tachometer-alt"></i> Performance Issues</h2><div class="performance-issue"><strong>[GC_OVERHEAD]</strong> High GC overhead in executor 2 (16.0%)<br><em>Recommendation:</em> Review memory usage patterns and consider increasing spark.memory.fraction if persistent<br><em>Severity:</em> 0.16 | <em>Affected Stages:</em> N/A</div><div class="performance-issue"><strong>[GC_OVERHEAD]</strong> High GC overhead in executor 1 (13.3%)<br><em>Recommendation:</em> Review memory usage patterns and consider increasing spark.memory.fraction if persistent<br><em>Severity:</em> 0.13 | <em>Affected Stages:</em> N/A</div><div class="performance-issue"><strong>[GC_OVERHEAD]</strong> High GC overhead in executor 3 (13.3%)<br><em>Recommendation:</em> Review memory usage patterns and consider increasing spark.memory.fraction if persistent<br><em>Severity:</em> 0.13 | <em>Affected Stages:</em> N/A</div></section><section id="bottlenecks" class="section"><h2>Bottleneck Jobs</h2><div>No bottlenecks detected.</div></section><section id="failures" class="section"><h2>Failed Jobs</h2><div>No failed jobs.</div></section><section id="recommendations" class="section"><h2>Recommendations</h2><div class="recommendation">(sev=0.60) [PYSPARK-MEDIUM] Executor 1 has high GC overhead (13.3% of execution time): Optimize memory usage and GC settings to reduce garbage collection overhead.</div><div class="recommendation">(sev=0.60) [PYSPARK-MEDIUM] Executor 2 has high GC overhead (16.0% of execution time): Optimize memory usage and GC settings to reduce garbage collection overhead.</div><div class="recommendation">(sev=0.60) [PYSPARK-MEDIUM] Executor 3 has high GC overhead (13.3% of execution time): Optimize memory usage and GC settings to reduce garbage collection overhead.</div><div class="recommendation">(sev=0.58) [GC_OVERHEAD] High GC overhead in executor 2 (16.0%): Review memory usage patterns and consider increasing spark.memory.fraction if persistent | Config: --conf spark.executor.memory=8g, --conf spark.memory.fraction=0.6</div><div class="recommendation">(sev=0.57) [GC_OVERHEAD] High GC overhead in executor 1 (13.3%): Review memory usage patterns and consider increasing spark.memory.fraction if persistent | Config: --conf spark.executor.memory=8g, --conf spark.memory.fraction=0.6</div><div class="recommendation">(sev=0.57) [GC_OVERHEAD] High GC overhead in executor 3 (13.3%): Review memory usage patterns and consider increasing spark.memory.fraction if persistent | Config: --conf spark.executor.memory=8g, --conf spark.memory.fraction=0.6</div></section><section id="jobs" class="section"><h2>All Jobs</h2><table class="job-table"><tr><th>Job ID</th><th>Status</th><th>Start Time</th><th>End Time</th><th>Duration (s)</th><th>Description</th><th>Query</th></tr><tr><td>0</td><td>JobSucceeded</td><td>2021-08-03 15:30:01</td><td>2021-08-03 15:30:06</td><td>5.00 s</td><td>collect at Main.scala:88</td><td><code></code></td></tr><tr><td>1</td><td>JobSucceeded</td><td>2021-08-03 15:30:07</td><td>2021-08-03 15:30:17</td><td>10.00 s</td><td>join at Analytics.scala:45</td><td><code>SELECT t1.*, t2.* FROM table1 t1 JOIN table2 t2 ON t1.id = t2.id WHERE t1.value > 1000</code></td></tr><tr><td>2</td><td>JobSucceeded</td><td>2021-08-03 15:30:18</td><td>2021-08-03 15:30:32</td><td>14.00 s</td><td>aggregate at Analysis.scala:67</td><td><code>SELECT category, AVG(value) as avg_value, COUNT(*) as count FROM large_table WHERE region = 'APAC' GROUP BY category HAVING COUNT(*) > 1000 ORDER BY avg_value DESC</code></td></tr></table></section></div></body></html>