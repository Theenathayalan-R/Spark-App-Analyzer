<!DOCTYPE html><html><head><meta charset="utf-8"><title>Spark Application Analysis Report</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js"></script><style>body { font-family: Inter, Arial, sans-serif; background: #f8f9fa; color: #222; margin: 0; padding: 0; }.navbar { background: #fff; padding: 1rem 2rem; box-shadow: 0 2px 4px rgba(0,0,0,0.07); position: sticky; top: 0; z-index: 100; }.nav-links { display: flex; gap: 2rem; margin-top: 1rem; }.nav-links a { color: #222; text-decoration: none; font-weight: 500; padding: 0.5rem 1rem; border-radius: 8px; transition: background 0.2s; }.nav-links a:hover { background: #e3eafc; }.container { max-width: 1200px; margin: 2rem auto; background: #fff; border-radius: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); padding: 2rem; }.summary-cards { display: flex; gap: 2rem; margin-bottom: 2rem; }.summary-card { flex: 1; background: #f5f6fa; border-radius: 8px; padding: 1.5rem; box-shadow: 0 1px 2px rgba(0,0,0,0.03); text-align: center; }.section { margin-bottom: 2.5rem; }.section h2 { margin-top: 0; }.bottleneck, .failed, .slow { background: #fff3cd; border-left: 4px solid #e67e22; margin: 1rem 0; padding: 1rem; border-radius: 6px; }.recommendation { background: #e3eafc; border-left: 4px solid #3498db; margin: 1rem 0; padding: 1rem; border-radius: 6px; }.pyspark-issue { background: #f0f8ff; border-left: 4px solid #4169e1; margin: 1rem 0; padding: 1.5rem; border-radius: 6px; }.pyspark-issue h4 { margin: 0 0 10px 0; color: #4169e1; }.severity-high { border-left-color: #ff4444; background: #ffe6e6; }.severity-medium { border-left-color: #ff8800; background: #fff2e6; }.severity-low { border-left-color: #44aa44; background: #e6ffe6; }.code-fix { background: #f8f8f8; border: 1px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px; font-family: "Fira Code", "Consolas", monospace; font-size: 13px; overflow-x: auto; }.job-table { width: 100%; border-collapse: collapse; margin-top: 1rem; }.job-table th, .job-table td { border: 1px solid #eee; padding: 0.5rem 0.75rem; text-align: left; }.job-table th { background: #f5f6fa; }.performance-issue { background: #fff9e6; border-left: 4px solid #ffa500; margin: 1rem 0; padding: 1rem; border-radius: 6px; }</style></head><body><div class="navbar"><h1><i class="fas fa-chart-line"></i> Spark Application Analysis</h1><div class="nav-links"><a href="#summary">Summary</a><a href="#pyspark-issues">PySpark Issues</a><a href="#performance-issues">Performance Issues</a><a href="#bottlenecks">Bottlenecks</a><a href="#failures">Failures</a><a href="#recommendations">Recommendations</a></div></div><div class="container"><section id="summary" class="section"><h2>Summary</h2><div class="summary-cards"><div class="summary-card"><h3>Total Jobs</h3><div>1</div></div><div class="summary-card"><h3>Avg Duration</h3><div>1.60 s</div></div><div class="summary-card"><h3>Success Rate</h3><div>0.0%</div></div><div class="summary-card"><h3>PySpark Issues</h3><div>3</div></div><div class="summary-card"><h3>Performance Issues</h3><div>5</div></div></div></section><section id="pyspark-issues" class="section"><h2><i class="fab fa-python"></i> PySpark Code Issues</h2><div class="pyspark-issue severity-high"><h4>[HIGH] Excessive Shuffle Spill</h4><p><strong>Problem:</strong> Stage 10 has excessive shuffle spill (5912.8 MB)</p><p><strong>Impact:</strong> High - Reduces disk I/O and improves shuffle performance</p><p><strong>Fix:</strong> Optimize shuffle operations to reduce memory pressure.</p><div class="code-fix"><pre><code class="language-python"># Reduce shuffle spill:
spark.conf.set("spark.executor.memory", "8g")

spark.conf.set("spark.shuffle.memoryFraction", "0.4")

df.repartition(200, "key_column")  # Better partition distribution

from pyspark.sql.functions import broadcast
large_df.join(broadcast(small_df), "key")</code></pre></div></div><div class="pyspark-issue severity-high"><h4>[HIGH] High Gc Overhead</h4><p><strong>Problem:</strong> Executor execA has high GC overhead (42.9% of execution time)</p><p><strong>Impact:</strong> High - Reduces CPU overhead from garbage collection</p><p><strong>Fix:</strong> Optimize memory usage and GC settings to reduce garbage collection overhead.</p><div class="code-fix"><pre><code class="language-python"># Reduce GC overhead:
spark.conf.set("spark.executor.memory", "8g")

spark.conf.set("spark.memory.fraction", "0.8")
spark.conf.set("spark.memory.storageFraction", "0.3")

spark.conf.set("spark.executor.extraJavaOptions", "-XX:+UseG1GC -XX:MaxGCPauseMillis=200")

df.mapPartitions(lambda partition: process_partition(partition))  # Process in batches</code></pre></div></div><div class="pyspark-issue severity-high"><h4>[HIGH] High Gc Overhead</h4><p><strong>Problem:</strong> Executor execB has high GC overhead (49.6% of execution time)</p><p><strong>Impact:</strong> High - Reduces CPU overhead from garbage collection</p><p><strong>Fix:</strong> Optimize memory usage and GC settings to reduce garbage collection overhead.</p><div class="code-fix"><pre><code class="language-python"># Reduce GC overhead:
spark.conf.set("spark.executor.memory", "8g")

spark.conf.set("spark.memory.fraction", "0.8")
spark.conf.set("spark.memory.storageFraction", "0.3")

spark.conf.set("spark.executor.extraJavaOptions", "-XX:+UseG1GC -XX:MaxGCPauseMillis=200")

df.mapPartitions(lambda partition: process_partition(partition))  # Process in batches</code></pre></div></div></section><section id="performance-issues" class="section"><h2><i class="fas fa-tachometer-alt"></i> Performance Issues</h2><div class="performance-issue"><strong>[MEMORY_PRESSURE_MODEL]</strong> Stage 10 shows high spill vs shuffle volume (spill_ratio=1.55, avg_gc=46.23%)<br><em>Recommendation:</em> Increase executor memory / tune partitions; consider caching & adjust spark.memory.fraction<br><em>Severity:</em> 1.00 | <em>Affected Stages:</em> 10</div><div class="performance-issue"><strong>[DATA_SKEW]</strong> Stage 10 shows significant data skew (CV: 0.80)<br><em>Recommendation:</em> Medium priority: Consider using range partitioning or adjusting spark.sql.adaptive.skewJoin.enabled=true<br><em>Severity:</em> 0.80 | <em>Affected Stages:</em> 10</div><div class="performance-issue"><strong>[SHUFFLE_SPILL]</strong> High shuffle spill in stage 10 (5.77 GB)<br><em>Recommendation:</em> Increase spark.shuffle.memoryFraction; Consider reducing partition count or implementing partial aggregation<br><em>Severity:</em> 0.62 | <em>Affected Stages:</em> 10</div><div class="performance-issue"><strong>[GC_OVERHEAD]</strong> High GC overhead in executor execB (49.6%)<br><em>Recommendation:</em> Critical: Increase executor memory, adjust spark.memory.fraction, and consider using G1GC with -XX:+UseG1GC<br><em>Severity:</em> 0.50 | <em>Affected Stages:</em> N/A</div><div class="performance-issue"><strong>[GC_OVERHEAD]</strong> High GC overhead in executor execA (42.9%)<br><em>Recommendation:</em> Critical: Increase executor memory, adjust spark.memory.fraction, and consider using G1GC with -XX:+UseG1GC<br><em>Severity:</em> 0.43 | <em>Affected Stages:</em> N/A</div></section><section id="bottlenecks" class="section"><h2>Bottleneck Jobs</h2><div>No bottlenecks detected.</div></section><section id="failures" class="section"><h2>Failed Jobs</h2><div class="failed"><b>Job 10</b>: Start: 1970-01-01 05:30:01, End: 1970-01-01 05:30:02, Duration: 1.60 s, Status: JobFailed, Desc: job_perf at pipeline.py:20</div></section><section id="recommendations" class="section"><h2>Recommendations</h2><div class="recommendation">(sev=1.00) [MEMORY_PRESSURE_MODEL] Stage 10 shows high spill vs shuffle volume (spill_ratio=1.55, avg_gc=46.23%): Increase executor memory / tune partitions; consider caching & adjust spark.memory.fraction</div><div class="recommendation">(sev=0.90) [PYSPARK-HIGH] Stage 10 has excessive shuffle spill (5912.8 MB): Optimize shuffle operations to reduce memory pressure.</div><div class="recommendation">(sev=0.90) [PYSPARK-HIGH] Executor execA has high GC overhead (42.9% of execution time): Optimize memory usage and GC settings to reduce garbage collection overhead.</div><div class="recommendation">(sev=0.90) [PYSPARK-HIGH] Executor execB has high GC overhead (49.6% of execution time): Optimize memory usage and GC settings to reduce garbage collection overhead.</div><div class="recommendation">(sev=0.90) [DATA_SKEW] Stage 10 shows significant data skew (CV: 0.80): Medium priority: Consider using range partitioning or adjusting spark.sql.adaptive.skewJoin.enabled=true | Config: spark.sql.adaptive.enabled=true, spark.sql.adaptive.skewJoin.enabled=true</div><div class="recommendation">(sev=0.81) [SHUFFLE_SPILL] High shuffle spill in stage 10 (5.77 GB): Increase spark.shuffle.memoryFraction; Consider reducing partition count or implementing partial aggregation | Config: spark.shuffle.sort.bypassMergeThreshold=200, spark.reducer.maxSizeInFlight=96m</div><div class="recommendation">(sev=0.80) [FAILURES] Some jobs failed. Check logs for errors and consider increasing executor memory or reviewing input data quality.</div><div class="recommendation">(sev=0.75) [GC_OVERHEAD] High GC overhead in executor execB (49.6%): Critical: Increase executor memory, adjust spark.memory.fraction, and consider using G1GC with -XX:+UseG1GC | Config: --conf spark.executor.memory=8g, --conf spark.memory.fraction=0.6</div><div class="recommendation">(sev=0.71) [GC_OVERHEAD] High GC overhead in executor execA (42.9%): Critical: Increase executor memory, adjust spark.memory.fraction, and consider using G1GC with -XX:+UseG1GC | Config: --conf spark.executor.memory=8g, --conf spark.memory.fraction=0.6</div></section><section id="code-mapping" class="section"><h2>Code Mapping</h2><div>No code repository mappings found.</div></section><section id="jobs" class="section"><h2>All Jobs</h2><table class="job-table"><tr><th>Job ID</th><th>Status</th><th>Start Time</th><th>End Time</th><th>Duration (s)</th><th>Description</th><th>Query</th></tr><tr><td>10</td><td>JobFailed</td><td>1970-01-01 05:30:01</td><td>1970-01-01 05:30:02</td><td>1.60 s</td><td>job_perf at pipeline.py:20</td><td><code></code></td></tr></table></section></div></body></html>